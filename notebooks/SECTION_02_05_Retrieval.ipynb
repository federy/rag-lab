{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5182128-f68a-4a0a-9ac0-e6c20814157a",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892105ce-f8b6-4cd5-80c8-0abca5987b05",
   "metadata": {},
   "source": [
    "## Load API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b38784-25e1-46d8-8d18-26181922d170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b8d13-009e-4589-a593-215b90675e31",
   "metadata": {},
   "source": [
    "## Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e223b2f-4045-446a-8393-f678211a26a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.openai import AsyncOpenAI  # autoinstrumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdbf19d-9643-464a-9645-893865380296",
   "metadata": {},
   "source": [
    "## Setup OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cff8e79-588c-4586-90ea-fa556824be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f622e028-248d-411c-be07-ceb07cbb983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_MODEL = \"text-embedding-3-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7e4e8c0-2c7b-467d-8237-c14daf0627c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT4O_MINI = \"gpt-4o-mini-2024-07-18\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b68359b-c532-47c9-a0df-d0dee3048054",
   "metadata": {},
   "source": [
    "## LLM Call Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5395f99a-db24-4871-bb17-583a104e56de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _msg(role, content):\n",
    "    return {'role': role, 'content': content}\n",
    "\n",
    "def system(content):\n",
    "    return _msg('system', content)\n",
    "\n",
    "def user(content):\n",
    "    return _msg('user', content)\n",
    "\n",
    "def assistant(content):\n",
    "    return _msg('assistant', content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9b04b1-8c94-4772-b1fd-b27bd42914a3",
   "metadata": {},
   "source": [
    "## Embedding Call Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cac8492d-92da-44ff-ace3-e4137754d38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(e) -> list[float]:\n",
    "    return e.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d07d56-92b6-4fae-a338-b58bfe7b8d32",
   "metadata": {},
   "source": [
    "## Compute Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50069418-a7eb-4e52-8668-5360459ace49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from openai.types.create_embedding_response import CreateEmbeddingResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0b79b71-5a7c-455a-ab3f-afdcd98bca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_cosine_sim(e1: CreateEmbeddingResponse, e2: CreateEmbeddingResponse) -> float:\n",
    "    e1, e2 = get_embedding(e1), get_embedding(e2)\n",
    "    to_np = lambda e: np.array(e).reshape(1, -1)\n",
    "    e1, e2 = to_np(e1), to_np(e2)\n",
    "    _cos_sim = cosine_similarity(e1, e2)\n",
    "    return _cos_sim[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faf523b-5001-456a-972a-537e9b85bfbd",
   "metadata": {},
   "source": [
    "## Cache System "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e974b152-059a-4927-af65-ca3e4fad9864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diskcache import Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12234460-2cf8-44ae-b47e-299340c3e711",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = Cache(directory=\".cache_course\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a964f6a8-f2fb-4990-892e-f754d0aee253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cce28600-6e67-4857-a970-6dca9ee307bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def set_async(key, val, **kwargs):\n",
    "    return await asyncio.to_thread(cache.set, key, val, **kwargs)\n",
    "\n",
    "async def get_async(key, default=None, **kwargs):\n",
    "    return await asyncio.to_thread(cache.get, key, default, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e834cfcf-56b1-4519-8abf-b7f316b99294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from hashlib import md5\n",
    "\n",
    "def make_cache_key(key_name, **kwargs):\n",
    "    kwargs_string = json.dumps(kwargs, sort_keys=True)\n",
    "    kwargs_hash = md5(kwargs_string.encode('utf-8')).hexdigest()\n",
    "    cache_key = f\"{key_name}__{kwargs_hash}\"\n",
    "    return cache_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d45d042-aa12-41f8-9415-1b5b98c30a85",
   "metadata": {},
   "source": [
    "## [EMBEDDING] Cached and Retried Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95eec4f7-c72a-4adc-8226-b769d6742db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "def _make_key_for_cached_embedding_with_retry(\n",
    "    *,\n",
    "    model,\n",
    "    input,\n",
    "    **kwargs,\n",
    "):\n",
    "    return make_cache_key(\n",
    "        \"openai_parsed_chat\",\n",
    "        model=model,\n",
    "        input=input,\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75621a9d-a4ba-46f8-ba63-cf544a12fbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.create_embedding_response import CreateEmbeddingResponse\n",
    "from functools import wraps\n",
    "from openai import APITimeoutError, RateLimitError\n",
    "from pydantic import BaseModel\n",
    "import backoff\n",
    "\n",
    "\n",
    "CACHE_MISS_SENTINEL = object()\n",
    "\n",
    "\n",
    "@wraps(client.embeddings.create)\n",
    "async def cached_embedding_with_retry(\n",
    "    *,\n",
    "    model,\n",
    "    input,\n",
    "    **kwargs,\n",
    ") -> CreateEmbeddingResponse:\n",
    "    # CREATE CACHE KEY\n",
    "    cache_key = _make_key_for_cached_embedding_with_retry(\n",
    "        model=model,\n",
    "        input=input,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    cached_value = await get_async(cache_key, default=CACHE_MISS_SENTINEL)\n",
    "    # CACHE MISS\n",
    "    if cached_value is CACHE_MISS_SENTINEL:\n",
    "        @backoff.on_exception(\n",
    "            backoff.expo,\n",
    "            (APITimeoutError, RateLimitError)\n",
    "        )\n",
    "        async def do_call():\n",
    "            return await client.embeddings.create(\n",
    "                model=model,\n",
    "                input=input,\n",
    "                **kwargs\n",
    "            )\n",
    "        embedding = await do_call()\n",
    "        await set_async(cache_key, embedding.json())\n",
    "        return embedding\n",
    "    # CACHE HIT\n",
    "    else:\n",
    "        embedding = CreateEmbeddingResponse.validate(json.loads(cached_value))\n",
    "        return embedding\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03140cb5-dbf0-4384-acd4-30f83f37f07e",
   "metadata": {},
   "source": [
    "## [LLM] Cached, Retried, and Traced Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04f735b0-8844-4608-bcdd-17a46e0ae135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "def _make_key_for_cached_chat_completion_parsed_with_retry(\n",
    "    *,\n",
    "    model,\n",
    "    messages,\n",
    "    response_format: BaseModel,\n",
    "    **kwargs,\n",
    "):\n",
    "    return make_cache_key(\n",
    "        \"openai_parsed_chat\",\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_format=response_format.model_json_schema(),\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cec357b6-49ad-4649-93cd-12ab44a682e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat import ParsedChatCompletion\n",
    "from functools import wraps\n",
    "from openai import APITimeoutError, RateLimitError\n",
    "from pydantic import BaseModel\n",
    "from typing_extensions import TypeVar\n",
    "import backoff\n",
    "\n",
    "ResponseFormatT = TypeVar(\"ResponseFormatT\", bound=BaseModel)\n",
    "\n",
    "CACHE_MISS_SENTINEL = object()\n",
    "\n",
    "\n",
    "@wraps(client.chat.completions.parse)\n",
    "async def cached_chat_completion_parsed_with_retry(\n",
    "    *,\n",
    "    model,\n",
    "    messages,\n",
    "    response_format: ResponseFormatT,\n",
    "    **kwargs,\n",
    ") -> ParsedChatCompletion[ResponseFormatT]:\n",
    "    # CREATE CACHE KEY\n",
    "    cache_key = _make_key_for_cached_chat_completion_parsed_with_retry(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_format=response_format,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    cached_value = await get_async(cache_key, default=CACHE_MISS_SENTINEL)\n",
    "    # CACHE MISS\n",
    "    if cached_value is CACHE_MISS_SENTINEL:\n",
    "        @backoff.on_exception(\n",
    "            backoff.expo,\n",
    "            (APITimeoutError, RateLimitError)\n",
    "        )\n",
    "        async def do_call():\n",
    "            return await client.chat.completions.parse(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                response_format=response_format,\n",
    "                **kwargs\n",
    "            )\n",
    "        completion = await do_call()\n",
    "        await set_async(cache_key, completion.model_dump_json())\n",
    "        return completion\n",
    "    # CACHE HIT\n",
    "    else:\n",
    "        # TODO: Tracing Code (next section)\n",
    "        # return \n",
    "        completion = ParsedChatCompletion.model_validate(json.loads(cached_value))\n",
    "        for choice in completion.choices:\n",
    "            if not choice.message.refusal:\n",
    "                choice.message.parsed = response_format.model_validate(\n",
    "                    choice.message.parsed\n",
    "                )\n",
    "        return completion\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b075e092-e285-4935-b883-b2f306dbdc95",
   "metadata": {},
   "source": [
    "## Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76779c2c-c47d-4d4f-af35-f1acf57d9d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "embedding = await cached_embedding_with_retry(\n",
    "    input=\"input: 'Union[str, List[str], Iterable[int], Iterable[Iterable[int]]]'\",\n",
    "    model=EMBED_MODEL\n",
    ")\n",
    "embedding_cosine_sim(embedding, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821503f1-e528-42db-bd2d-397924c8928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "completion = await cached_chat_completion_parsed_with_retry(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n",
    "    ],\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "event = completion.choices[0].message.parsed\n",
    "event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6a2938-b278-4efa-b94b-f4a205526f07",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec7f0419-6d85-415a-a4ed-f8b174446c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rewritten_questions.json', 'r') as f:\n",
    "    questions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a21ccdb1-13b9-43f4-9b72-52e42ede0d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emails = pd.read_csv('paul_allen_sent_email_with_questions_v1.csv')\n",
    "\n",
    "del emails['questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ccb8031-90a1-46a0-97c6-a6da7fde685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "c_ = CONSTANTS = SimpleNamespace(\n",
    "    MESSAGE_ID='Message-ID',\n",
    "    USELESS_TO_RECALL='useless_to_recall',\n",
    "    CONTENT='content',\n",
    "    DATE='Date',\n",
    "    CORRESPONDANTS='Correspondants',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14365d0a-b35c-468b-aef3-51584c218502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      <13537630.1075855669909.JavaMail.evans@thyme\n",
       "1      <27903020.1075855669931.JavaMail.evans@thyme\n",
       "2      <12929996.1075855668941.JavaMail.evans@thyme\n",
       "3      <29770699.1075855669609.JavaMail.evans@thyme\n",
       "4      <17449361.1075855672476.JavaMail.evans@thyme\n",
       "                           ...                     \n",
       "391     <2313514.1075855693911.JavaMail.evans@thyme\n",
       "392    <10598636.1075855693867.JavaMail.evans@thyme\n",
       "393     <5195408.1075855693846.JavaMail.evans@thyme\n",
       "394     <7510478.1075855693794.JavaMail.evans@thyme\n",
       "395    <20840552.1075855693485.JavaMail.evans@thyme\n",
       "Name: Message-ID, Length: 396, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails[c_.MESSAGE_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b6dc0-f19e-416c-b8fd-0b0a7ed43652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-course",
   "language": "python",
   "name": "rag-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
