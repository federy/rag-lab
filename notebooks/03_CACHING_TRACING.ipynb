{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d03aff5e-4626-4c29-adb5-6a0cccf7aefd",
   "metadata": {},
   "source": [
    "# Get API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e4bf905-8e64-4f94-9da5-a8de7d5c4e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5dadde-367b-41c4-812e-aafcb2e8d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import AsyncOpenAI\n",
    "from langfuse.openai import AsyncOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "996a4d01-314c-4dce-9002-fee1ec9ac0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f60a868-7cd0-4d54-bdf1-306b3407cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT4O_MINI = \"o4-mini-2025-04-16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e751ba5-8207-43c4-8f3a-e3a7928c2dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _msg(role, content):\n",
    "    return {'role': role, 'content': content}\n",
    "\n",
    "def system(content):\n",
    "    return _msg('system', content)\n",
    "\n",
    "def user(content):\n",
    "    return _msg('user', content)\n",
    "\n",
    "def assistant(content):\n",
    "    return _msg('assistant', content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f8c397-fcfe-444d-8779-d28cd33cb910",
   "metadata": {},
   "source": [
    "# Cache to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "594ad9d0-a67b-410d-8c2c-8691bdfbc129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diskcache import Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f4f7551-fd6d-427c-b7f5-afa7455644a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache = Cache() # temporary cache\n",
    "cache = Cache(directory=\".cache_course\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e255e8a7-2c19-408d-999f-4af9628ce9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cache.set(\"Hello\", \"blabla\")\n",
    "#cache.get(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f98b746-23bb-4306-bd35-f324e6810ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc0e456f-67b2-47ca-95c8-494a6fcd0183",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def set_async(key, val, **kwargs):\n",
    "    return await asyncio.to_thread(cache.set, key, val, **kwargs)\n",
    "\n",
    "async def get_async(key, default=None, **kwargs):\n",
    "    return await asyncio.to_thread(cache.get, key, default, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f45ecd9a-d34f-4b1a-8d2f-9193117f6271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NOT FOUND', 'blabla', True, 'yes')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity checks\n",
    "(\n",
    "    await get_async(\"key_that_does_not_exist\", default=\"NOT FOUND\"),\n",
    "    await get_async(\"Hello\"),\n",
    "    await set_async(\"key_exist\", \"yes\"),\n",
    "    await get_async(\"key_exist\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171b6c6a-1cfe-476a-a785-94d21a0d87ee",
   "metadata": {},
   "source": [
    "# LLMs calls are cached"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d25ca93-ff10-41f0-bd15-75701dbd2ae0",
   "metadata": {},
   "source": [
    "<h2>Concept</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dc3c5c8-5192-4615-86a2-29bbb8274009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'48818a53e154bcdfc356cff1fbab9ae1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashlib import md5\n",
    "\n",
    "md5(b\"dqsdsqdqs\").hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d2dc82d-5da7-4414-a294-27c08b607189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2']{'a': 2, 'b': 4}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'8fe21d329868d5b6aef0d2d118a97c62'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashlib import md5\n",
    "\n",
    "args = [\"1\", \"2\"]\n",
    "kwargs = dict(a=2, b=4)\n",
    "\n",
    "dirty = str(args) + str(kwargs)\n",
    "\n",
    "print(dirty)\n",
    "\n",
    "md5(dirty.encode('utf-8')).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db58d1e2-68e3-4077-b201-939633eef05c",
   "metadata": {},
   "source": [
    "<h2>Why is it dirty?</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8973d36-1942-4fbe-8be7-397a8bdc7a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]{'a': 2, 'b': 4}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'61b079025bfa60a4e712ebef7d78bd0e'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashlib import md5\n",
    "\n",
    "args = [1, 2]\n",
    "kwargs = dict(a=2, b=4)\n",
    "\n",
    "dirty = str(args) + str(kwargs)\n",
    "\n",
    "print(dirty)\n",
    "\n",
    "md5(dirty.encode('utf-8')).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d49fb85-bb2f-45ab-86ec-081d2bbd1830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 4]{}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0c6d1d9f15baeb020e7541e36250390d'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashlib import md5\n",
    "\n",
    "args = [1, 2, 2, 4]\n",
    "kwargs = dict()\n",
    "\n",
    "dirty = str(args) + str(kwargs)\n",
    "\n",
    "print(dirty)\n",
    "\n",
    "md5(dirty.encode('utf-8')).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0496228-2504-4c2c-ae6e-8a55c5b130f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]{'b': 4, 'a': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'7d78d95310185eb7a6a2805760b58e56'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashlib import md5\n",
    "\n",
    "args = [1, 2]\n",
    "kwargs = dict(b=4, a=2)\n",
    "\n",
    "dirty = str(args) + str(kwargs)\n",
    "\n",
    "print(dirty)\n",
    "\n",
    "md5(dirty.encode('utf-8')).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36b862fb-6d99-43a7-af7d-6591ec4135a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(arg1, arg2, a, b):\n",
    "    pass\n",
    "\n",
    "func(1, 2, 3, 4)\n",
    "func(1, 2, a=1, b=2)\n",
    "func(1, 2, b=2, a=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13505ed7-3bb4-4fe8-a790-af4d95629bdb",
   "metadata": {},
   "source": [
    "<h3>are equivalent</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b079d32-24ca-452d-b00a-03f0c421d17a",
   "metadata": {},
   "source": [
    "<h3>BUT there are different hashes for the same arguments!</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821c605b-023d-4e8b-b845-e74edcea61d6",
   "metadata": {},
   "source": [
    "# Clean way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e8e8171-14d1-4573-8d96-5cd5709a0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def make_cache_key(key_name, **kwargs):\n",
    "    kwargs_string = json.dumps(kwargs, sort_keys=True)\n",
    "    kwargs_hash = md5(kwargs_string.encode('utf-8')).hexdigest()\n",
    "    cache_key = f\"{key_name}__{kwargs_hash}\"\n",
    "    \n",
    "    return cache_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78aa79ef-4cca-4201-863a-ba99481d07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_cache_key_for_chat_completion(\n",
    "    *,\n",
    "    model,\n",
    "    messages,\n",
    "    **kwargs,\n",
    "):\n",
    "    return make_cache_key(\n",
    "        \"openai_chat_completion\",\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65bd17fe-8616-41b0-9a86-fa01840b72d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'openai_chat_completion__45981e772eec9f1d6b6e8c28511644ae'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_make_cache_key_for_chat_completion(\n",
    "    messages=[1, 2, 3],\n",
    "    model=GPT4O_MINI\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0b3e6ea-fd52-46ce-9be9-87ed6059cfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat import ChatCompletion\n",
    "from functools import update_wrapper\n",
    "\n",
    "CACHE_MISS_SENTINEL = object()\n",
    "\n",
    "async def cached_chat_completion(\n",
    "    *,\n",
    "    model,\n",
    "    messages,\n",
    "    **kwargs,\n",
    ") -> ChatCompletion:\n",
    "    # CREATE CAHE KEY\n",
    "    cache_key = _make_cache_key_for_chat_completion(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        **kwargs\n",
    "    )\n",
    "    cached_value = await get_async(cache_key, default=CACHE_MISS_SENTINEL)\n",
    "    # CACHE MISS\n",
    "    if cached_value is CACHE_MISS_SENTINEL:\n",
    "        completion = await client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            **kwargs\n",
    "        )\n",
    "        await set_async(cache_key, completion.json())\n",
    "        return completion\n",
    "    # CACHE HIT\n",
    "    else:\n",
    "        return ChatCompletion.validate(json.loads(cached_value))\n",
    "\n",
    "cached_chat_completion = update_wrapper(\n",
    "    cached_chat_completion,\n",
    "    wrapped=client.chat.completions.create\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87146e2d-e47e-48c5-81f4-0030516ff653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_913732/2528705122.py:30: PydanticDeprecatedSince20: The `validate` method is deprecated; use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  return ChatCompletion.validate(json.loads(cached_value))\n"
     ]
    }
   ],
   "source": [
    "completion = await cached_chat_completion(\n",
    "    messages=[user(\"What is caching in software engineering?\")],\n",
    "    model=GPT4O_MINI,\n",
    "    max_completion_tokens=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "177577e0-dd92-4fdc-8678-ff487f34eb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-C1eYDHrAK4n3R9YIbJYYgRUZXnzgA', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering is the practice of storing copies of data or computation results in a faster‐access storage layer (the “cache”) so that future requests for that data can be served more quickly than by going back to the original, usually slower, source (disk, database, remote service, etc.).\\n\\n1. Purpose and Benefits  \\n  • Reduced latency: fetching data from memory or a nearby cache is much faster than from a database or over the network.  \\n  • Lower backend load: repeated requests hit the cache instead of overwhelming the primary data store.  \\n  • Increased throughput and scalability: systems can handle more requests per second when much of the work is offloaded to caches.\\n\\n2. Common Cache Levels and Types  \\n  • CPU caches (L1, L2, L3): hardware‐level caches that store recent instructions and data.  \\n  • In‐process or in‐memory caches (e.g. in a web server): store results of expensive function calls or database queries.  \\n  • Distributed caches (e.g. Redis, Memcached): share a cache across multiple application servers.  \\n  • HTTP/browser caches and CDNs: cache web resources (HTML, images, API responses) closer to end users or in the browser.\\n\\n3. Eviction and Consistency Strategies  \\n  • Eviction policies decide which entries to evict when the cache is full:  \\n    – LRU (Least Recently Used)  \\n    – LFU (Least Frequently Used)  \\n    – FIFO (First In, First Out)  \\n  • Cache invalidation/expiration: ensure data doesn’t go stale by setting time‐to‐live (TTL) or explicitly invalidating entries on updates.  \\n\\n4. Write Policies  \\n  • Write‐Through: data is written to cache and backing store synchronously.  \\n  • Write‐Back (Write‐Behind): data is written only to cache first, and later flushed to the backing store.  \\n  • Write‐Around: writes go directly to the backing store, leaving the cache unchanged until the next read.\\n\\n5. Trade-Offs and Challenges  \\n  • Stale data: unless properly invalidated, caches can serve out-of-date information.  \\n  • Cache warming: after a restart or cache flush, performance may dip until the cache repopulates.  \\n  • Complexity: adds another layer to design, debugging, and failure modes (e.g. cache misses, stampedes).\\n\\nIn short, caching is a fundamental performance‐optimization technique in software systems. By carefully choosing what to cache, how long to keep it, and how to keep it consistent, developers can dramatically speed up applications and reduce load on slower resources.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1754510241, model='o4-mini-2025-04-16', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=882, prompt_tokens=13, total_tokens=895, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=320, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6921fb38-e23b-4640-9253-9275cf346847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_913732/2528705122.py:30: PydanticDeprecatedSince20: The `validate` method is deprecated; use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  return ChatCompletion.validate(json.loads(cached_value))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-C2MzTwheZzcG9rxo7baz7WJcouUjT', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering is the practice of storing copies of data or computations in a temporary, fast‐access storage layer (the “cache”) so that future requests for the same data can be served more quickly. Instead of recomputing a result or fetching data from a slower, more expensive source every time, the system checks the cache first.\\n\\nKey points about caching:\\n\\n1. Purpose  \\n  • Reduce latency by avoiding repeated expensive operations (disk I/O, database queries, complex calculations).  \\n  • Lower backend load (database, remote service).  \\n  • Improve throughput and user‐perceived performance.\\n\\n2. Common cache types  \\n  • In‐memory caches (e.g., Redis, Memcached)  \\n  • On‐disk or local file caches (e.g., browser HTTP cache, local disk cache for files)  \\n  • CPU caches (L1/L2/L3 caches inside processors)  \\n  • Distributed caches (shared across multiple application servers)\\n\\n3. Cache lifecycle  \\n  • Populate: first-time request misses cache (“cache miss”), data is fetched/computed and stored in cache.  \\n  • Retrieve: subsequent requests hit the cache (“cache hit”) and return data quickly.  \\n  • Expire/Eviction: entries are removed based on time‐to‐live (TTL) policies or capacity limits.\\n\\n4. Eviction/Replacement strategies  \\n  • LRU (Least Recently Used): evict the data least recently accessed.  \\n  • LFU (Least Frequently Used): evict the data accessed least often.  \\n  • FIFO (First In, First Out).  \\n  • Custom policies (size‐based, priority‐based).\\n\\n5. Consistency & Invalidation  \\n  • Cache invalidation is one of the hardest problems: keeping cache coherent with the authoritative data source.  \\n  • Strategies include time‐based expiration (TTL), write‐through (write to both cache and database), write‐back (write to cache and defer database write), or explicit invalidation on updates.\\n\\n6. Trade-offs & pitfalls  \\n  • Stale data risk if invalidation isn’t handled.  \\n  • Increased system complexity (more components to monitor and debug).  \\n  • Memory overhead—caches consume resources.  \\n  • Cache stampede (many requests simultaneously miss a stale cache and hit the backend) mitigated via locking or request coalescing.\\n\\n7. Typical use cases  \\n  • Web applications caching HTML fragments or API responses.  \\n  • Database query result caching.  \\n  • Content Delivery Networks (CDNs) caching static assets.  \\n  • Session storage.  \\n  • Computation results (e.g., memoization in code).\\n\\nBest practices  \\n  – Define clear cache keys (include versioning if your data model changes).  \\n  – Set sensible TTLs based on how fresh the data must be.  \\n  – Monitor hit/miss ratios to tune cache size and policies.  \\n  – Graceful fallback to the source when caches fail or miss.  \\n  – Consider layered caching (e.g., in‐process + distributed) for maximum efficiency.\\n\\nIn essence, caching trades memory (or disk) for speed, helping applications scale and respond faster by reusing previously fetched or computed data.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1754681067, model='o4-mini-2025-04-16', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=802, prompt_tokens=13, total_tokens=815, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=128, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = await cached_chat_completion(\n",
    "    messages=[user(\"What is caching in software engineering??\")],\n",
    "    model=GPT4O_MINI,\n",
    "    max_completion_tokens=1000\n",
    ")\n",
    "\n",
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee5e84c3-ad98-4e50-9835-c091d737023f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m\n",
       "cached_chat_completion(\n",
       "    *,\n",
       "    messages: \u001b[33m'Iterable[ChatCompletionMessageParam]'\u001b[39m,\n",
       "    model: \u001b[33m'Union[str, ChatModel]'\u001b[39m,\n",
       "    audio: \u001b[33m'Optional[ChatCompletionAudioParam] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    frequency_penalty: \u001b[33m'Optional[float] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    function_call: \u001b[33m'completion_create_params.FunctionCall | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    functions: \u001b[33m'Iterable[completion_create_params.Function] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    logit_bias: \u001b[33m'Optional[Dict[str, int]] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    logprobs: \u001b[33m'Optional[bool] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    max_completion_tokens: \u001b[33m'Optional[int] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    max_tokens: \u001b[33m'Optional[int] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    metadata: \u001b[33m'Optional[Metadata] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    modalities: \u001b[33m\"Optional[List[Literal['text', 'audio']]] | NotGiven\"\u001b[39m = NOT_GIVEN,\n",
       "    n: \u001b[33m'Optional[int] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    parallel_tool_calls: \u001b[33m'bool | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    prediction: \u001b[33m'Optional[ChatCompletionPredictionContentParam] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    presence_penalty: \u001b[33m'Optional[float] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    prompt_cache_key: \u001b[33m'str | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    reasoning_effort: \u001b[33m'Optional[ReasoningEffort] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    response_format: \u001b[33m'completion_create_params.ResponseFormat | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    safety_identifier: \u001b[33m'str | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    seed: \u001b[33m'Optional[int] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    service_tier: \u001b[33m\"Optional[Literal['auto', 'default', 'flex', 'scale', 'priority']] | NotGiven\"\u001b[39m = NOT_GIVEN,\n",
       "    stop: \u001b[33m'Union[Optional[str], List[str], None] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    store: \u001b[33m'Optional[bool] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    stream: \u001b[33m'Optional[Literal[False]] | Literal[True] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    stream_options: \u001b[33m'Optional[ChatCompletionStreamOptionsParam] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    temperature: \u001b[33m'Optional[float] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    tool_choice: \u001b[33m'ChatCompletionToolChoiceOptionParam | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    tools: \u001b[33m'Iterable[ChatCompletionToolParam] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    top_logprobs: \u001b[33m'Optional[int] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    top_p: \u001b[33m'Optional[float] | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    user: \u001b[33m'str | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    web_search_options: \u001b[33m'completion_create_params.WebSearchOptions | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       "    extra_headers: \u001b[33m'Headers | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    extra_query: \u001b[33m'Query | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    extra_body: \u001b[33m'Body | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    timeout: \u001b[33m'float | httpx.Timeout | None | NotGiven'\u001b[39m = NOT_GIVEN,\n",
       ") -> \u001b[33m'ChatCompletion | AsyncStream[ChatCompletionChunk]'\u001b[39m\n",
       "\u001b[31mDocstring:\u001b[39m <no docstring>\n",
       "\u001b[31mFile:\u001b[39m      ~/LIVE/course/.venv/lib64/python3.12/site-packages/openai/resources/chat/completions/completions.py\n",
       "\u001b[31mType:\u001b[39m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cached_chat_completion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663d5a97-ad1a-498d-bb60-2cd2e67e96d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-course",
   "language": "python",
   "name": "rag-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
