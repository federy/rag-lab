{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5182128-f68a-4a0a-9ac0-e6c20814157a",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892105ce-f8b6-4cd5-80c8-0abca5987b05",
   "metadata": {},
   "source": [
    "## Load API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b38784-25e1-46d8-8d18-26181922d170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b8d13-009e-4589-a593-215b90675e31",
   "metadata": {},
   "source": [
    "## Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e223b2f-4045-446a-8393-f678211a26a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.openai import AsyncOpenAI  # autoinstrmenttion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdbf19d-9643-464a-9645-893865380296",
   "metadata": {},
   "source": [
    "## Setup OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cff8e79-588c-4586-90ea-fa556824be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f622e028-248d-411c-be07-ceb07cbb983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_MODEL = \"text-embedding-3-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7e4e8c0-2c7b-467d-8237-c14daf0627c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT4O_MINI = \"o4-mini-2025-04-16\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b68359b-c532-47c9-a0df-d0dee3048054",
   "metadata": {},
   "source": [
    "## LLM Call Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5395f99a-db24-4871-bb17-583a104e56de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _msg(role, content):\n",
    "    return {'role': role, 'content': content}\n",
    "\n",
    "def system(content):\n",
    "    return _msg('system', content)\n",
    "\n",
    "def user(content):\n",
    "    return _msg('user', content)\n",
    "\n",
    "def assistant(content):\n",
    "    return _msg('assistant', content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9b04b1-8c94-4772-b1fd-b27bd42914a3",
   "metadata": {},
   "source": [
    "## Embedding Call Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cac8492d-92da-44ff-ace3-e4137754d38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(e) -> list[float]:\n",
    "    return e.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d07d56-92b6-4fae-a338-b58bfe7b8d32",
   "metadata": {},
   "source": [
    "## Compute Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50069418-a7eb-4e52-8668-5360459ace49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from openai.types.create_embedding_response import CreateEmbeddingResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0b79b71-5a7c-455a-ab3f-afdcd98bca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_cosine_sim(e1: CreateEmbeddingResponse, e2: CreateEmbeddingResponse) -> float:\n",
    "    e1, e2 = get_embedding(e1), get_embedding(e2)\n",
    "    to_np = lambda e: np.array(e).reshape(1, -1)\n",
    "    e1, e2 = to_np(e1), to_np(e2)\n",
    "    _cos_sim = cosine_similarity(e1, e2)\n",
    "    return _cos_sim[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faf523b-5001-456a-972a-537e9b85bfbd",
   "metadata": {},
   "source": [
    "## Cache System "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e974b152-059a-4927-af65-ca3e4fad9864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diskcache import Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12234460-2cf8-44ae-b47e-299340c3e711",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = Cache(directory=\".cache_course\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a964f6a8-f2fb-4990-892e-f754d0aee253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cce28600-6e67-4857-a970-6dca9ee307bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def set_async(key, val, **kwargs):\n",
    "    return await asyncio.to_thread(cache.set, key, val, **kwargs)\n",
    "\n",
    "async def get_async(key, default=None, **kwargs):\n",
    "    return await asyncio.to_thread(cache.get, key, default, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e834cfcf-56b1-4519-8abf-b7f316b99294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from hashlib import md5\n",
    "\n",
    "def make_cache_key(key_name, **kwargs):\n",
    "    kwargs_string = json.dumps(kwargs, sort_keys=True)\n",
    "    kwargs_hash = md5(kwargs_string.encode('utf-8')).hexdigest()\n",
    "    cache_key = f\"{key_name}__{kwargs_hash}\"\n",
    "    return cache_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d45d042-aa12-41f8-9415-1b5b98c30a85",
   "metadata": {},
   "source": [
    "## [EMBEDDING] Cached and Retried Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95eec4f7-c72a-4adc-8226-b769d6742db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "def _make_key_for_cached_embedding_with_retry(\n",
    "    *,\n",
    "    model,\n",
    "    input,\n",
    "    **kwargs,\n",
    "):\n",
    "    return make_cache_key(\n",
    "        \"openai_parsed_chat\",\n",
    "        model=model,\n",
    "        input=input,\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75621a9d-a4ba-46f8-ba63-cf544a12fbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.create_embedding_response import CreateEmbeddingResponse\n",
    "from functools import wraps\n",
    "from openai import APITimeoutError, RateLimitError\n",
    "from pydantic import BaseModel\n",
    "import backoff\n",
    "\n",
    "\n",
    "CACHE_MISS_SENTINEL = object()\n",
    "\n",
    "\n",
    "@wraps(client.embeddings.create)\n",
    "async def cached_embedding_with_retry(\n",
    "    *,\n",
    "    model,\n",
    "    input,\n",
    "    **kwargs,\n",
    ") -> CreateEmbeddingResponse:\n",
    "    # CREATE CACHE KEY\n",
    "    cache_key = _make_key_for_cached_embedding_with_retry(\n",
    "        model=model,\n",
    "        input=input,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    cached_value = await get_async(cache_key, default=CACHE_MISS_SENTINEL)\n",
    "    # CACHE MISS\n",
    "    if cached_value is CACHE_MISS_SENTINEL:\n",
    "        @backoff.on_exception(\n",
    "            backoff.expo,\n",
    "            (APITimeoutError, RateLimitError)\n",
    "        )\n",
    "        async def do_call():\n",
    "            return await client.embeddings.create(\n",
    "                model=model,\n",
    "                input=input,\n",
    "                **kwargs\n",
    "            )\n",
    "        embedding = await do_call()\n",
    "        await set_async(cache_key, embedding.model_dump_json())\n",
    "        return embedding\n",
    "    # CACHE HIT\n",
    "    else:\n",
    "        embedding = CreateEmbeddingResponse.model_validate(json.loads(cached_value))\n",
    "        return embedding\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03140cb5-dbf0-4384-acd4-30f83f37f07e",
   "metadata": {},
   "source": [
    "## [LLM] Cached, Retried, and Traced Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04f735b0-8844-4608-bcdd-17a46e0ae135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "def _make_key_for_cached_chat_completion_parsed_with_retry(\n",
    "    *,\n",
    "    model,\n",
    "    messages,\n",
    "    response_format: BaseModel,\n",
    "    **kwargs,\n",
    "):\n",
    "    return make_cache_key(\n",
    "        \"openai_parsed_chat\",\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_format=response_format.model_json_schema(),\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cec357b6-49ad-4649-93cd-12ab44a682e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat import ParsedChatCompletion\n",
    "from functools import wraps\n",
    "from openai import APITimeoutError, RateLimitError\n",
    "from pydantic import BaseModel\n",
    "from typing_extensions import TypeVar\n",
    "import backoff\n",
    "\n",
    "ResponseFormatT = TypeVar(\"ResponseFormatT\", bound=BaseModel)\n",
    "\n",
    "CACHE_MISS_SENTINEL = object()\n",
    "\n",
    "\n",
    "@wraps(client.chat.completions.parse)\n",
    "async def cached_chat_completion_parsed_with_retry(\n",
    "    *,\n",
    "    model,\n",
    "    messages,\n",
    "    response_format: ResponseFormatT,\n",
    "    **kwargs,\n",
    ") -> ParsedChatCompletion[ResponseFormatT]:\n",
    "    # CREATE CACHE KEY\n",
    "    cache_key = _make_key_for_cached_chat_completion_parsed_with_retry(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_format=response_format,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    cached_value = await get_async(cache_key, default=CACHE_MISS_SENTINEL)\n",
    "    # CACHE MISS\n",
    "    if cached_value is CACHE_MISS_SENTINEL:\n",
    "        @backoff.on_exception(\n",
    "            backoff.expo,\n",
    "            (APITimeoutError, RateLimitError)\n",
    "        )\n",
    "        async def do_call():\n",
    "            return await client.chat.completions.parse(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                response_format=response_format,\n",
    "                **kwargs\n",
    "            )\n",
    "        completion = await do_call()\n",
    "        await set_async(cache_key, completion.model_dump_json())\n",
    "        return completion\n",
    "    # CACHE HIT\n",
    "    else:\n",
    "        # TODO: Tracing Code (next section)\n",
    "        # return \n",
    "        completion = ParsedChatCompletion.model_validate(json.loads(cached_value))\n",
    "        for choice in completion.choices:\n",
    "            if not choice.message.refusal:\n",
    "                choice.message.parsed = response_format.model_validate(\n",
    "                    choice.message.parsed\n",
    "                )\n",
    "        return completion\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b075e092-e285-4935-b883-b2f306dbdc95",
   "metadata": {},
   "source": [
    "## Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76779c2c-c47d-4d4f-af35-f1acf57d9d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "embedding = await cached_embedding_with_retry(\n",
    "    input=\"input: 'Union[str, List[str], Iterable[int], Iterable[Iterable[int]]]'\",\n",
    "    model=EMBED_MODEL\n",
    ")\n",
    "embedding_cosine_sim(embedding, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821503f1-e528-42db-bd2d-397924c8928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "completion = await cached_chat_completion_parsed_with_retry(\n",
    "    model=GPT4O_MINI,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n",
    "    ],\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "event = completion.choices[0].message.parsed\n",
    "event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6a2938-b278-4efa-b94b-f4a205526f07",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fb4b62-84a7-4734-aa16-4762b3c147ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('deduplicated_questions.json', 'r') as f:\n",
    "    questions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b976528-3059-430a-82eb-f6fb05f58101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emails = pd.read_csv('paul_allen_sent_email_with_questions_v1.csv')\n",
    "\n",
    "emails.set_index(\"Message-ID\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc164a78-283e-4886-9fd0-f090be100895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class RewriteQuestionsSchema(BaseModel):\n",
    "    concise_reasoning: str\n",
    "    should_rewrite: bool\n",
    "    rewritten_questions: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478c2775-0ddb-401f-85eb-42cfeba4347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa5a335-59c9-474b-90f4-c26486731c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = dedent(\n",
    "    \"\"\"\\\n",
    "    Your task is to assess if a question could stand alone in a retrieval system that searches through thousands of emails, then to rewrite it in a way that it is if it isn't.\n",
    "\n",
    "    You'll answer in JSON by respecting the following schema:\n",
    "    ```ts\n",
    "    {\n",
    "        concise_reasoning: str\n",
    "        should_rewrite: true | false\n",
    "        rewritten_question: str\n",
    "    }\n",
    "    ```\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeda6b29-70c0-473e-890d-dd1466c41cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = Template(dedent(\n",
    "    \"\"\"\\\n",
    "    Based on the following email sent to the following correspondant\n",
    "\n",
    "    <correspondants>\n",
    "    {{ correspondants }}\n",
    "    </correspondants>\n",
    "\n",
    "    <email>\n",
    "    {{ email }}\n",
    "    </email>\n",
    "\n",
    "    Can the following question stand alone in a retrieval system that searches through thousands of emails?\n",
    "\n",
    "    For example:\n",
    "    - If it mentions \"in the email\", it won't stand alone as this is not specific enough\n",
    "    - If it's a direct question but it doesn't include information about the email the question is from, then we likely won't be able to retrieve the associated email\n",
    "\n",
    "    <question>\n",
    "    {{ question }}\n",
    "    </question>\n",
    "    \n",
    "    If the question doesn't stand alone, rewrite it so that it is, for example you can add \"In the email about ...,\" in front of the question.\n",
    "\n",
    "    First concisely reason step by step if the question is standalone or need rewriting, then decide if it should be rewritten.\n",
    "    If it should be, then rewrite it.\n",
    "    \"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd612ee-ec2b-48ea-ac43-cf2eea57909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import asyncio\n",
    "\n",
    "semaphore = asyncio.Semaphore(50)  # max 50 requêtes simultanées\n",
    "\n",
    "LLM_MODEL = GPT4O_MINI\n",
    "#LLM_MODEL = \"gpt-5\"\n",
    "\n",
    "async def rewrite_question(prompt):\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            results = await cached_chat_completion_parsed_with_retry(\n",
    "                model=LLM_MODEL,\n",
    "                messages=[\n",
    "                    system(system_prompt),\n",
    "                    user(prompt)\n",
    "                ],\n",
    "                max_completion_tokens=5000,\n",
    "                temperature=1.,\n",
    "                response_format=RewriteQuestionsSchema\n",
    "            )\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            return (prompt, traceback.format_exc, e)\n",
    "\n",
    "async def limited_call(prompt):\n",
    "    async with semaphore:\n",
    "        return await rewrite_question(prompt)\n",
    "\n",
    "tasks = []\n",
    "\n",
    "for (email_id, question)  in questions:\n",
    "    email = emails.loc[email_id].content\n",
    "    correspondants = emails.loc[email_id].Correspondants\n",
    "\n",
    "    prompt = prompt_template.render(\n",
    "        email=email,\n",
    "        correspondants=correspondants,\n",
    "        question=question\n",
    "    )\n",
    "\n",
    "    tasks.append(\n",
    "        rewrite_question(\n",
    "            prompt\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90468c47-be4f-4c54-b847-1df9162cbe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c3fba-eeab-439b-83be-18914fb0eaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "results = await tqdm_asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54015ac8-91bf-4b53-a31b-2d945479f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = [r for r in results if isinstance(r, tuple)]\n",
    "len(failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ae4a5-0cda-4331-b600-d5252daa35f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "new_questions = deepcopy(questions)\n",
    "\n",
    "for i in range(len(questions)):\n",
    "    if results[i].choices[0].message.parsed.should_rewrite:\n",
    "        new_questions[i][1] = results[i].choices[0].message.parsed.rewritten_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab53d1da-0286-481d-94a9-af810051bfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rewritten_questions.json', 'w') as f:\n",
    "    json.dump(new_questions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023c5dc6-36b9-4a92-bad5-0611e3af8a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rewritten_questions.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d0bc53-892b-49d6-84d1-791a5e5d986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data == new_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6e3bde-ab25-4b2c-8084-b542c0f7412d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-course",
   "language": "python",
   "name": "rag-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
