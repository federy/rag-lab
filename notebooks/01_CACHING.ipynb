{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d03aff5e-4626-4c29-adb5-6a0cccf7aefd",
   "metadata": {},
   "source": [
    "# Get API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e4bf905-8e64-4f94-9da5-a8de7d5c4e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5dadde-367b-41c4-812e-aafcb2e8d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "996a4d01-314c-4dce-9002-fee1ec9ac0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f60a868-7cd0-4d54-bdf1-306b3407cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT4O_MINI = \"o4-mini-2025-04-16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e751ba5-8207-43c4-8f3a-e3a7928c2dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _msg(role, content):\n",
    "    return {'role': role, 'content': content}\n",
    "\n",
    "def system(content):\n",
    "    return _msg('system', content)\n",
    "\n",
    "def user(content):\n",
    "    return _msg('user', content)\n",
    "\n",
    "def assistant(content):\n",
    "    return _msg('assistant', content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a5fa41-dcfa-44ea-9e43-7dcecb2cf863",
   "metadata": {},
   "source": [
    "# Not Cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0385ea2-22ee-4041-9d85-ca70d3e53e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "\n",
    "completion = await client.chat.completions.create(\n",
    "    messages=[user(\"What is caching in software engineering?\")],\n",
    "    model=GPT4O_MINI,\n",
    "    max_completion_tokens=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba2f3b2d-dff8-472e-aacd-13621a297188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Caching in software engineering is the practice of storing copies of data or '\n",
      " 'computations in a “fast‐access” storage layer so that future requests for '\n",
      " 'the same data can be served more quickly, avoid repeated work, and reduce '\n",
      " 'load on slower or more expensive resources (databases, file systems, remote '\n",
      " 'services, etc.).\\n'\n",
      " '\\n'\n",
      " 'Key aspects of caching:\\n'\n",
      " '\\n'\n",
      " '1. What gets cached  \\n'\n",
      " '   • Computed results (e.g., function return values, rendered templates)  \\n'\n",
      " '   • Database query results or ORM objects  \\n'\n",
      " '   • HTTP responses or fragments (pages, images, JSON)  \\n'\n",
      " '   • Files or blobs (on-disk cache for downloads)  \\n'\n",
      " '\\n'\n",
      " '2. Where caches live  \\n'\n",
      " '   • In-process memory (application-level caches)  \\n'\n",
      " '   • Dedicated cache servers (Redis, Memcached)  \\n'\n",
      " '   • Content Delivery Networks (CDNs) and reverse proxies (e.g., Varnish, '\n",
      " 'Nginx)  \\n'\n",
      " '   • Browser cache or operating‐system file cache  \\n'\n",
      " '\\n'\n",
      " '3. Cache policies and mechanics  \\n'\n",
      " '   – Time-to-Live (TTL) or expiration: how long an entry stays valid.  \\n'\n",
      " '   – Eviction policies: choose which entries to drop when space is needed. '\n",
      " 'Common algorithms include  \\n'\n",
      " '     • LRU (Least Recently Used)  \\n'\n",
      " '     • LFU (Least Frequently Used)  \\n'\n",
      " '     • FIFO (First In, First Out)  \\n'\n",
      " '     • Random or custom heuristics  \\n'\n",
      " '   – Write strategies (for caches that can be written to):  \\n'\n",
      " '     • Write-through: every update goes to cache and backing store '\n",
      " 'immediately  \\n'\n",
      " '     • Write-back (lazy write): updates go to cache only and are '\n",
      " 'periodically flushed to the backing store  \\n'\n",
      " '   – Cache warming/preloading: populating the cache ahead of time with '\n",
      " 'anticipated data  \\n'\n",
      " '\\n'\n",
      " '4. Benefits  \\n'\n",
      " '   • Lower latency for end users  \\n'\n",
      " '   • Reduced load on origin servers, databases, or compute resources  \\n'\n",
      " '   • Improved throughput and scalability of applications  \\n'\n",
      " '\\n'\n",
      " '5. Challenges and pitfalls  \\n'\n",
      " '   • Stale data and cache invalidation (“one of the two hardest problems in '\n",
      " 'computer science”)  \\n'\n",
      " '   • Consistency models in distributed caches (trade-offs between freshness '\n",
      " 'and performance)  \\n'\n",
      " '   • Cache stampedes (many clients missing the cache simultaneously)  \\n'\n",
      " '     – Mitigation: request coalescing, locks, “early recompute”  \\n'\n",
      " '   • Memory pressure and cache pollution  \\n'\n",
      " '\\n'\n",
      " '6. Typical use-cases  \\n'\n",
      " '   • Web applications: page and API response caching  \\n'\n",
      " '   • Microservices: client-side or shared caches for service responses  \\n'\n",
      " '   • Databases: query result caches or materialized views  \\n'\n",
      " '   • Computation-heavy tasks: memoization of pure‐function outputs  \\n'\n",
      " '   • File systems and OS kernels: page caches to speed up disk I/O  \\n'\n",
      " '\\n'\n",
      " 'In essence, caching is about trading a small amount of fast, limited‐size '\n",
      " 'storage and added complexity for much better performance and scalability in '\n",
      " 'software systems.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f8c397-fcfe-444d-8779-d28cd33cb910",
   "metadata": {},
   "source": [
    "# Cache to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "594ad9d0-a67b-410d-8c2c-8691bdfbc129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diskcache import Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f4f7551-fd6d-427c-b7f5-afa7455644a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache = Cache() # temporary cache\n",
    "cache = Cache(directory=\".cache_course\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e255e8a7-2c19-408d-999f-4af9628ce9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cache.set(\"Hello\", \"blabla\")\n",
    "#cache.get(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f98b746-23bb-4306-bd35-f324e6810ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc0e456f-67b2-47ca-95c8-494a6fcd0183",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def set_async(key, val, **kwargs):\n",
    "    return await asyncio.to_thread(cache.set, key, val, **kwargs)\n",
    "\n",
    "async def get_async(key, default=None, **kwargs):\n",
    "    return await asyncio.to_thread(cache.get, key, default, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f45ecd9a-d34f-4b1a-8d2f-9193117f6271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NOT FOUND', 'blabla', True, 'yes')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity checks\n",
    "(\n",
    "    await get_async(\"key_that_does_not_exist\", default=\"NOT FOUND\"),\n",
    "    await get_async(\"Hello\"),\n",
    "    await set_async(\"key_exist\", \"yes\"),\n",
    "    await get_async(\"key_exist\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171b6c6a-1cfe-476a-a785-94d21a0d87ee",
   "metadata": {},
   "source": [
    "# LLMs calls are cached"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d25ca93-ff10-41f0-bd15-75701dbd2ae0",
   "metadata": {},
   "source": [
    "<h2>Concept</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2dc3c5c8-5192-4615-86a2-29bbb8274009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'48818a53e154bcdfc356cff1fbab9ae1'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashlib import md5\n",
    "\n",
    "md5(b\"dqsdsqdqs\").hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d2dc82d-5da7-4414-a294-27c08b607189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2']{'a': 2, 'b': 4}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'8fe21d329868d5b6aef0d2d118a97c62'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashlib import md5\n",
    "\n",
    "args = [\"1\", \"2\"]\n",
    "kwargs = dict(a=2, b=4)\n",
    "\n",
    "dirty = str(args) + str(kwargs)\n",
    "\n",
    "print(dirty)\n",
    "\n",
    "md5(dirty.encode('utf-8')).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db58d1e2-68e3-4077-b201-939633eef05c",
   "metadata": {},
   "source": [
    "<h2>Why is it dirty?</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8973d36-1942-4fbe-8be7-397a8bdc7a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]{'a': 2, 'b': 4}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'61b079025bfa60a4e712ebef7d78bd0e'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashlib import md5\n",
    "\n",
    "args = [1, 2]\n",
    "kwargs = dict(a=2, b=4)\n",
    "\n",
    "dirty = str(args) + str(kwargs)\n",
    "\n",
    "print(dirty)\n",
    "\n",
    "md5(dirty.encode('utf-8')).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d49fb85-bb2f-45ab-86ec-081d2bbd1830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 4]{}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0c6d1d9f15baeb020e7541e36250390d'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashlib import md5\n",
    "\n",
    "args = [1, 2, 2, 4]\n",
    "kwargs = dict()\n",
    "\n",
    "dirty = str(args) + str(kwargs)\n",
    "\n",
    "print(dirty)\n",
    "\n",
    "md5(dirty.encode('utf-8')).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0496228-2504-4c2c-ae6e-8a55c5b130f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]{'b': 4, 'a': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'7d78d95310185eb7a6a2805760b58e56'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashlib import md5\n",
    "\n",
    "args = [1, 2]\n",
    "kwargs = dict(b=4, a=2)\n",
    "\n",
    "dirty = str(args) + str(kwargs)\n",
    "\n",
    "print(dirty)\n",
    "\n",
    "md5(dirty.encode('utf-8')).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36b862fb-6d99-43a7-af7d-6591ec4135a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(arg1, arg2, a, b):\n",
    "    pass\n",
    "\n",
    "func(1, 2, 3, 4)\n",
    "func(1, 2, a=1, b=2)\n",
    "func(1, 2, b=2, a=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13505ed7-3bb4-4fe8-a790-af4d95629bdb",
   "metadata": {},
   "source": [
    "<h3>are equivalent</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b079d32-24ca-452d-b00a-03f0c421d17a",
   "metadata": {},
   "source": [
    "<h3>BUT there are different hashes for the same arguments!</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821c605b-023d-4e8b-b845-e74edcea61d6",
   "metadata": {},
   "source": [
    "# Clean way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e8e8171-14d1-4573-8d96-5cd5709a0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def make_cache_key(key_name, **kwargs):\n",
    "    kwargs_string = json.dumps(kwargs, sort_keys=True)\n",
    "    kwargs_hash = md5(kwargs_string.encode('utf-8')).hexdigest()\n",
    "    cache_key = f\"{key_name}__{kwargs_hash}\"\n",
    "    \n",
    "    return cache_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78aa79ef-4cca-4201-863a-ba99481d07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_cache_key_for_chat_completion(\n",
    "    *,\n",
    "    model,\n",
    "    messages,\n",
    "    **kwargs,\n",
    "):\n",
    "    return make_cache_key(\n",
    "        \"openai_chat_completion\",\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65bd17fe-8616-41b0-9a86-fa01840b72d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'openai_chat_completion__45981e772eec9f1d6b6e8c28511644ae'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_make_cache_key_for_chat_completion(\n",
    "    messages=[1, 2, 3],\n",
    "    model=GPT4O_MINI\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e0b3e6ea-fd52-46ce-9be9-87ed6059cfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat import ChatCompletion\n",
    "\n",
    "CACHE_MISS_SENTINEL = object()\n",
    "\n",
    "async def cached_chat_completion(\n",
    "    *,\n",
    "    model,\n",
    "    messages,\n",
    "    **kwargs,\n",
    ") -> ChatCompletion:\n",
    "    # CREATE CAHE KEY\n",
    "    cache_key = _make_cache_key_for_chat_completion(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        **kwargs\n",
    "    )\n",
    "    cached_value = await get_async(cache_key, default=CACHE_MISS_SENTINEL)\n",
    "    # CACHE MISS\n",
    "    if cached_value is CACHE_MISS_SENTINEL:\n",
    "        completion = await client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            **kwargs\n",
    "        )\n",
    "        await set_async(cache_key, completion.json())\n",
    "        return completion\n",
    "    # CACHE HIT\n",
    "    else:\n",
    "        return ChatCompletion.validate(json.loads(cached_value))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8bb83d3e-b274-49f0-802e-e4b00367bfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat import ChatCompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e6d8cc49-1369-492c-b29d-0437afde42bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3987418/815953895.py:1: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  ChatCompletion.validate(json.loads(completion.json()))\n",
      "/tmp/ipykernel_3987418/815953895.py:1: PydanticDeprecatedSince20: The `validate` method is deprecated; use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  ChatCompletion.validate(json.loads(completion.json()))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-C1cG0k055MRTSG639cUcx9EWWSUTK', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering is the practice of storing copies of data or computations in a “fast‐access” storage layer so that future requests for the same data can be served more quickly, avoid repeated work, and reduce load on slower or more expensive resources (databases, file systems, remote services, etc.).\\n\\nKey aspects of caching:\\n\\n1. What gets cached  \\n   • Computed results (e.g., function return values, rendered templates)  \\n   • Database query results or ORM objects  \\n   • HTTP responses or fragments (pages, images, JSON)  \\n   • Files or blobs (on-disk cache for downloads)  \\n\\n2. Where caches live  \\n   • In-process memory (application-level caches)  \\n   • Dedicated cache servers (Redis, Memcached)  \\n   • Content Delivery Networks (CDNs) and reverse proxies (e.g., Varnish, Nginx)  \\n   • Browser cache or operating‐system file cache  \\n\\n3. Cache policies and mechanics  \\n   – Time-to-Live (TTL) or expiration: how long an entry stays valid.  \\n   – Eviction policies: choose which entries to drop when space is needed. Common algorithms include  \\n     • LRU (Least Recently Used)  \\n     • LFU (Least Frequently Used)  \\n     • FIFO (First In, First Out)  \\n     • Random or custom heuristics  \\n   – Write strategies (for caches that can be written to):  \\n     • Write-through: every update goes to cache and backing store immediately  \\n     • Write-back (lazy write): updates go to cache only and are periodically flushed to the backing store  \\n   – Cache warming/preloading: populating the cache ahead of time with anticipated data  \\n\\n4. Benefits  \\n   • Lower latency for end users  \\n   • Reduced load on origin servers, databases, or compute resources  \\n   • Improved throughput and scalability of applications  \\n\\n5. Challenges and pitfalls  \\n   • Stale data and cache invalidation (“one of the two hardest problems in computer science”)  \\n   • Consistency models in distributed caches (trade-offs between freshness and performance)  \\n   • Cache stampedes (many clients missing the cache simultaneously)  \\n     – Mitigation: request coalescing, locks, “early recompute”  \\n   • Memory pressure and cache pollution  \\n\\n6. Typical use-cases  \\n   • Web applications: page and API response caching  \\n   • Microservices: client-side or shared caches for service responses  \\n   • Databases: query result caches or materialized views  \\n   • Computation-heavy tasks: memoization of pure‐function outputs  \\n   • File systems and OS kernels: page caches to speed up disk I/O  \\n\\nIn essence, caching is about trading a small amount of fast, limited‐size storage and added complexity for much better performance and scalability in software systems.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1754501424, model='o4-mini-2025-04-16', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=727, prompt_tokens=13, total_tokens=740, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=128, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatCompletion.validate(json.loads(completion.json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "87146e2d-e47e-48c5-81f4-0030516ff653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3987418/3530422529.py:25: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  await set_async(cache_key, completion.json())\n"
     ]
    }
   ],
   "source": [
    "completion = await cached_chat_completion(\n",
    "    messages=[user(\"What is caching in software engineering?\")],\n",
    "    model=GPT4O_MINI,\n",
    "    max_completion_tokens=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "177577e0-dd92-4fdc-8678-ff487f34eb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-C1eYDHrAK4n3R9YIbJYYgRUZXnzgA', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering is the practice of storing copies of data or computation results in a faster‐access storage layer (the “cache”) so that future requests for that data can be served more quickly than by going back to the original, usually slower, source (disk, database, remote service, etc.).\\n\\n1. Purpose and Benefits  \\n  • Reduced latency: fetching data from memory or a nearby cache is much faster than from a database or over the network.  \\n  • Lower backend load: repeated requests hit the cache instead of overwhelming the primary data store.  \\n  • Increased throughput and scalability: systems can handle more requests per second when much of the work is offloaded to caches.\\n\\n2. Common Cache Levels and Types  \\n  • CPU caches (L1, L2, L3): hardware‐level caches that store recent instructions and data.  \\n  • In‐process or in‐memory caches (e.g. in a web server): store results of expensive function calls or database queries.  \\n  • Distributed caches (e.g. Redis, Memcached): share a cache across multiple application servers.  \\n  • HTTP/browser caches and CDNs: cache web resources (HTML, images, API responses) closer to end users or in the browser.\\n\\n3. Eviction and Consistency Strategies  \\n  • Eviction policies decide which entries to evict when the cache is full:  \\n    – LRU (Least Recently Used)  \\n    – LFU (Least Frequently Used)  \\n    – FIFO (First In, First Out)  \\n  • Cache invalidation/expiration: ensure data doesn’t go stale by setting time‐to‐live (TTL) or explicitly invalidating entries on updates.  \\n\\n4. Write Policies  \\n  • Write‐Through: data is written to cache and backing store synchronously.  \\n  • Write‐Back (Write‐Behind): data is written only to cache first, and later flushed to the backing store.  \\n  • Write‐Around: writes go directly to the backing store, leaving the cache unchanged until the next read.\\n\\n5. Trade-Offs and Challenges  \\n  • Stale data: unless properly invalidated, caches can serve out-of-date information.  \\n  • Cache warming: after a restart or cache flush, performance may dip until the cache repopulates.  \\n  • Complexity: adds another layer to design, debugging, and failure modes (e.g. cache misses, stampedes).\\n\\nIn short, caching is a fundamental performance‐optimization technique in software systems. By carefully choosing what to cache, how long to keep it, and how to keep it consistent, developers can dramatically speed up applications and reduce load on slower resources.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1754510241, model='o4-mini-2025-04-16', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=882, prompt_tokens=13, total_tokens=895, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=320, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6921fb38-e23b-4640-9253-9275cf346847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3987418/3530422529.py:29: PydanticDeprecatedSince20: The `validate` method is deprecated; use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  return ChatCompletion.validate(json.loads(cached_value))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-C1eYDHrAK4n3R9YIbJYYgRUZXnzgA', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering is the practice of storing copies of data or computation results in a faster‐access storage layer (the “cache”) so that future requests for that data can be served more quickly than by going back to the original, usually slower, source (disk, database, remote service, etc.).\\n\\n1. Purpose and Benefits  \\n  • Reduced latency: fetching data from memory or a nearby cache is much faster than from a database or over the network.  \\n  • Lower backend load: repeated requests hit the cache instead of overwhelming the primary data store.  \\n  • Increased throughput and scalability: systems can handle more requests per second when much of the work is offloaded to caches.\\n\\n2. Common Cache Levels and Types  \\n  • CPU caches (L1, L2, L3): hardware‐level caches that store recent instructions and data.  \\n  • In‐process or in‐memory caches (e.g. in a web server): store results of expensive function calls or database queries.  \\n  • Distributed caches (e.g. Redis, Memcached): share a cache across multiple application servers.  \\n  • HTTP/browser caches and CDNs: cache web resources (HTML, images, API responses) closer to end users or in the browser.\\n\\n3. Eviction and Consistency Strategies  \\n  • Eviction policies decide which entries to evict when the cache is full:  \\n    – LRU (Least Recently Used)  \\n    – LFU (Least Frequently Used)  \\n    – FIFO (First In, First Out)  \\n  • Cache invalidation/expiration: ensure data doesn’t go stale by setting time‐to‐live (TTL) or explicitly invalidating entries on updates.  \\n\\n4. Write Policies  \\n  • Write‐Through: data is written to cache and backing store synchronously.  \\n  • Write‐Back (Write‐Behind): data is written only to cache first, and later flushed to the backing store.  \\n  • Write‐Around: writes go directly to the backing store, leaving the cache unchanged until the next read.\\n\\n5. Trade-Offs and Challenges  \\n  • Stale data: unless properly invalidated, caches can serve out-of-date information.  \\n  • Cache warming: after a restart or cache flush, performance may dip until the cache repopulates.  \\n  • Complexity: adds another layer to design, debugging, and failure modes (e.g. cache misses, stampedes).\\n\\nIn short, caching is a fundamental performance‐optimization technique in software systems. By carefully choosing what to cache, how long to keep it, and how to keep it consistent, developers can dramatically speed up applications and reduce load on slower resources.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1754510241, model='o4-mini-2025-04-16', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=882, prompt_tokens=13, total_tokens=895, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=320, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = await cached_chat_completion(\n",
    "    messages=[user(\"What is caching in software engineering?\")],\n",
    "    model=GPT4O_MINI,\n",
    "    max_completion_tokens=1000\n",
    ")\n",
    "\n",
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5e84c3-ad98-4e50-9835-c091d737023f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-course",
   "language": "python",
   "name": "rag-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
